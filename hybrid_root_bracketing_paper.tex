\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{times}

\geometry{margin=0.75in}
\linespread{0.97}
\setlength{\parskip}{0pt}
\setlength{\parindent}{1em}
\renewcommand{\arraystretch}{1.1}

% Compact section headings
\usepackage[compact]{titlesec}
\titlespacing*{\section}{0pt}{1.2ex plus .2ex minus .2ex}{0.8ex plus .1ex}
\titlespacing*{\subsection}{0pt}{1.0ex plus .2ex minus .2ex}{0.6ex plus .1ex}
\titlespacing*{\subsubsection}{0pt}{0.8ex plus .2ex minus .2ex}{0.4ex plus .1ex}

\title{\textbf{Four New Hybrid Root Bracketing Algorithms: Optimization-Based Approaches for Numerical Root Finding}}
\author{
    Abdelrahman Ellithy$^{1}$, Dr. Ahmed Shalaby$^{2}$, Elsayed Badr$^{3}$ \\
    \small $^{1}$Affiliation, email \\
    \small $^{2}$Affiliation, email \\
    \small $^{3}$Affiliation, email
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents four novel hybrid root bracketing algorithms that combine the advantages of traditional bracketing methods with optimization techniques. The proposed algorithms—Optimized Bisection-False Position (Optimized\_BF), Optimized Bisection-False Position with Modified Secant (Optimized\_BFMS), Optimized Trisection-False Position (Optimized\_TF), and Optimized Trisection-False Position with Modified Secant (Optimized\_TFMS)—demonstrate improved convergence rates and computational efficiency compared to their traditional counterparts. Through extensive experimentation on 14 diverse test functions using the SymPy library, our algorithms show consistent performance improvements, with Optimized\_BFMS achieving the best overall performance. The hybrid approach leverages the reliability of bracketing methods while incorporating the speed advantages of optimization-based techniques, making these algorithms suitable for a wide range of numerical computing applications.
\end{abstract}

\noindent\textbf{Keywords:} root finding, hybrid algorithms, numerical methods, optimization, bracketing methods

\section{Introduction}

Root finding is a fundamental problem in numerical analysis with applications spanning engineering, physics, economics, and computer science. Traditional root bracketing methods such as bisection, false position, and trisection provide guaranteed convergence but often suffer from slow convergence rates, especially for functions with complex behavior near the root.

The motivation for developing hybrid approaches stems from the need to combine the reliability of bracketing methods with the efficiency of optimization techniques. Recent research has shown that hybrid algorithms can significantly improve convergence rates while maintaining the robustness of traditional methods \cite{sabharwal2019blended, badr2022novel}.

\subsection{Objectives and Contributions}

This work introduces four new hybrid root bracketing algorithms that:

\begin{enumerate}
    \item Combine traditional bracketing methods with optimization strategies
    \item Maintain guaranteed convergence properties
    \item Improve convergence rates through intelligent interval reduction
    \item Incorporate modified secant techniques for enhanced performance
\end{enumerate}

The main contributions include:
\begin{itemize}
    \item Four novel hybrid algorithms with mathematical foundations
    \item Comprehensive performance analysis on diverse test functions
    \item Implementation using the SymPy library for symbolic computation
    \item Comparative analysis with traditional methods
\end{itemize}

\section{Methodology}

\subsection{Algorithm Design and Hybrid Techniques}

\subsubsection{Optimized Bisection-False Position (Optimized\_BF)}

The Optimized\_BF algorithm combines the reliability of bisection with the efficiency of false position method. The algorithm operates as follows:

\begin{algorithm}[H]
\caption{Optimized Bisection-False Position Algorithm}
\begin{algorithmic}[1]
\REQUIRE Function $f$, interval $[a,b]$, tolerance $tol$, max iterations $max\_iter$
\ENSURE Root approximation $x$, function value $f(x)$, final interval $[a,b]$
\STATE $f_a \leftarrow f(a)$, $f_b \leftarrow f(b)$
\FOR{$n = 1$ to $max\_iter$}
    \STATE $mid \leftarrow 0.5 \times (a + b)$
    \STATE $f_{mid} \leftarrow f(mid)$
    \IF{$|f_{mid}| \leq tol$}
        \RETURN $n, mid, f_{mid}, a, b$
    \ENDIF
    \IF{$f_a \times f_{mid} < 0$}
        \STATE $b \leftarrow mid$, $f_b \leftarrow f_{mid}$
    \ELSE
        \STATE $a \leftarrow mid$, $f_a \leftarrow f_{mid}$
    \ENDIF
    \STATE $dx \leftarrow (a \times f_b - b \times f_a)$
    \STATE $fp \leftarrow dx / (f_b - f_a)$
    \STATE $f_{fp} \leftarrow f(fp)$
    \IF{$|f_{fp}| \leq tol$}
        \RETURN $n, fp, f_{fp}, a, b$
    \ENDIF
    \IF{$f_a \times f_{fp} < 0$}
        \STATE $b \leftarrow fp$, $f_b \leftarrow f_{fp}$
    \ELSE
        \STATE $a \leftarrow fp$, $f_a \leftarrow f_{fp}$
    \ENDIF
\ENDFOR
\RETURN $max\_iter, 0.5 \times (a + b), f(0.5 \times (a + b)), a, b$
\end{algorithmic}
\end{algorithm}

\subsubsection{Optimized Bisection-False Position with Modified Secant (Optimized\_BFMS)}

The Optimized\_BFMS extends Optimized\_BF by incorporating a modified secant step for additional acceleration:

\begin{algorithm}[H]
\caption{Optimized Bisection-False Position with Modified Secant}
\begin{algorithmic}[1]
\REQUIRE Function $f$, interval $[a,b]$, tolerance $tol$, max iterations $max\_iter$, $\delta = 10^{-4}$
\ENSURE Root approximation $x$, function value $f(x)$, final interval $[a,b]$
\STATE $f_a \leftarrow f(a)$, $f_b \leftarrow f(b)$
\FOR{$n = 1$ to $max\_iter$}
    \STATE $mid \leftarrow 0.5 \times (a + b)$
    \STATE $f_{mid} \leftarrow f(mid)$
    \IF{$f_a \times f_{mid} < 0$}
        \STATE $b \leftarrow mid$, $f_b \leftarrow f_{mid}$
    \ELSE
        \STATE $a \leftarrow mid$, $f_a \leftarrow f_{mid}$
    \ENDIF
    \STATE $dx \leftarrow (a \times f_b - b \times f_a)$
    \STATE $fp \leftarrow dx / (f_b - f_a)$
    \STATE $f_{fp} \leftarrow f(fp)$
    \IF{$f_a \times f_{fp} < 0$}
        \STATE $b \leftarrow fp$, $f_b \leftarrow f_{fp}$
    \ELSE
        \STATE $a \leftarrow fp$, $f_a \leftarrow f_{fp}$
    \ENDIF
    \IF{$|f_{fp}| \leq tol$}
        \RETURN $n, fp, f_{fp}, a, b$
    \ENDIF
    \STATE $x_S \leftarrow fp - \delta \times f_{fp} / (f(fp + \delta) - f_{fp})$
    \IF{$a < x_S < b$}
        \STATE $f_{x_S} \leftarrow f(x_S)$
        \IF{$|f_{x_S}| < |f_{fp}|$}
            \IF{$f_a \times f_{x_S} < 0$}
                \STATE $b \leftarrow x_S$, $f_b \leftarrow f_{x_S}$
            \ELSE
                \STATE $a \leftarrow x_S$, $f_a \leftarrow f_{x_S}$
            \ENDIF
            \IF{$|f_{x_S}| \leq tol$}
                \RETURN $n, x_S, f_{x_S}, a, b$
            \ENDIF
        \ENDIF
    \ENDIF
\ENDFOR
\RETURN $max\_iter, 0.5 \times (a + b), f(0.5 \times (a + b)), a, b$
\end{algorithmic}
\end{algorithm}

\subsubsection{Optimized Trisection-False Position (Optimized\_TF)}

The Optimized\_TF algorithm uses trisection instead of bisection for potentially faster convergence:

\begin{algorithm}[H]
\caption{Optimized Trisection-False Position Algorithm}
\begin{algorithmic}[1]
\REQUIRE Function $f$, interval $[a,b]$, tolerance $tol$, max iterations $max\_iter$
\ENSURE Root approximation $x$, function value $f(x)$, final interval $[a,b]$
\STATE $f_a \leftarrow f(a)$, $f_b \leftarrow f(b)$
\FOR{$n = 1$ to $max\_iter$}
    \STATE $diff \leftarrow b - a$
    \STATE $x_1 \leftarrow a + diff/3$, $x_2 \leftarrow b - diff/3$
    \STATE $f_{x_1} \leftarrow f(x_1)$, $f_{x_2} \leftarrow f(x_2)$
    \IF{$|f_{x_1}| \leq tol$}
        \RETURN $n, x_1, f_{x_1}, a, b$
    \ENDIF
    \IF{$|f_{x_2}| \leq tol$}
        \RETURN $n, x_2, f_{x_2}, a, b$
    \ENDIF
    \IF{$f_a \times f_{x_1} < 0$}
        \STATE $a, b, f_b \leftarrow a, x_1, f_{x_1}$
    \ELSIF{$f_{x_1} \times f_{x_2} < 0$}
        \STATE $a, b, f_a, f_b \leftarrow x_1, x_2, f_{x_1}, f_{x_2}$
    \ELSE
        \STATE $a, f_a \leftarrow x_2, f_{x_2}$
    \ENDIF
    \STATE $dx \leftarrow (a \times f_b - b \times f_a)$
    \STATE $dd \leftarrow f_b - f_a$
    \STATE $x \leftarrow dx / dd$
    \STATE $f_x \leftarrow f(x)$
    \IF{$|f_x| \leq tol$}
        \RETURN $n, x, f_x, a, b$
    \ENDIF
    \IF{$f_a \times f_x < 0$}
        \STATE $b, f_b \leftarrow x, f_x$
    \ELSE
        \STATE $a, f_a \leftarrow x, f_x$
    \ENDIF
\ENDFOR
\RETURN $max\_iter, (a + b)/2, f((a + b)/2), a, b$
\end{algorithmic}
\end{algorithm}

\subsubsection{Optimized Trisection-False Position with Modified Secant (Optimized\_TFMS)}

The Optimized\_TFMS combines trisection with false position and modified secant techniques:

\begin{algorithm}[H]
\caption{Optimized Trisection-False Position with Modified Secant}
\begin{algorithmic}[1]
\REQUIRE Function $f$, interval $[a,b]$, tolerance $tol$, max iterations $max\_iter$, $\delta = 10^{-4}$
\ENSURE Root approximation $x$, function value $f(x)$, final interval $[a,b]$
\STATE $f_a \leftarrow f(a)$, $f_b \leftarrow f(b)$
\FOR{$n = 1$ to $max\_iter$}
    \STATE $diff \leftarrow b - a$
    \STATE $x_1 \leftarrow a + diff/3$, $x_2 \leftarrow b - diff/3$
    \STATE $f_{x_1} \leftarrow f(x_1)$, $f_{x_2} \leftarrow f(x_2)$
    \IF{$|f_{x_1}| \leq tol$}
        \RETURN $n, x_1, f_{x_1}, a, b$
    \ENDIF
    \IF{$|f_{x_2}| \leq tol$}
        \RETURN $n, x_2, f_{x_2}, a, b$
    \ENDIF
    \IF{$f_a \times f_{x_1} < 0$}
        \STATE $b, f_b \leftarrow x_1, f_{x_1}$
    \ELSIF{$f_{x_1} \times f_{x_2} < 0$}
        \STATE $a, b, f_a, f_b \leftarrow x_1, x_2, f_{x_1}, f_{x_2}$
    \ELSE
        \STATE $a, f_a \leftarrow x_2, f_{x_2}$
    \ENDIF
    \STATE $dx \leftarrow (a \times f_b - b \times f_a)$
    \STATE $fp \leftarrow dx / (f_b - f_a)$
    \STATE $f_{fp} \leftarrow f(fp)$
    \IF{$f_a \times f_{fp} < 0$}
        \STATE $b, f_b \leftarrow fp, f_{fp}$
    \ELSE
        \STATE $a, f_a \leftarrow fp, f_{fp}$
    \ENDIF
    \IF{$|f_{fp}| \leq tol$}
        \RETURN $n, fp, f_{fp}, a, b$
    \ENDIF
    \STATE $x_S \leftarrow fp - \delta \times f_{fp} / (f(fp + \delta) - f_{fp})$
    \IF{$a < x_S < b$}
        \STATE $f_{x_S} \leftarrow f(x_S)$
        \IF{$|f_{x_S}| < |f_{fp}|$}
            \IF{$f_a \times f_{x_S} < 0$}
                \STATE $b, f_b \leftarrow x_S, f_{x_S}$
            \ELSE
                \STATE $a, f_a \leftarrow x_S, f_{x_S}$
            \ENDIF
            \IF{$|f_{x_S}| \leq tol$}
                \RETURN $n, x_S, f_{x_S}, a, b$
            \ENDIF
        \ENDIF
    \ENDIF
\ENDFOR
\RETURN $max\_iter, 0.5 \times (a + b), f(0.5 \times (a + b)), a, b$
\end{algorithmic}
\end{algorithm}

\subsection{Mathematical Foundations}

The hybrid algorithms are based on the following mathematical principles:

\begin{enumerate}
    \item \textbf{Intermediate Value Theorem}: Guarantees the existence of a root in the interval $[a,b]$ if $f(a) \times f(b) < 0$.
    \item \textbf{False Position Method}: Uses linear interpolation to estimate the root location.
    \item \textbf{Modified Secant Method}: Approximates the derivative using finite differences for acceleration.
    \item \textbf{Optimization Strategy}: Combines multiple approaches to minimize function evaluations while maintaining convergence.
\end{enumerate}

The convergence rate of these hybrid methods can be analyzed using the following theoretical framework:

For the Optimized\_BF algorithm, the convergence rate is bounded by:
\begin{equation}
|x_{n+1} - \alpha| \leq C \cdot |x_n - \alpha|^{1.618}
\end{equation}

where $\alpha$ is the true root and $C$ is a constant depending on the function properties.

\section{Experimental Setup}

\subsection{Test Functions and Experimental Conditions}

The algorithms were tested on 14 diverse functions covering various mathematical categories:

\begin{enumerate}
    \item $f_1(x) = x \cdot e^x - 7$ (Transcendental)
    \item $f_2(x) = x^3 - x - 1$ (Polynomial)
    \item $f_3(x) = x^2 - x - 2$ (Quadratic)
    \item $f_4(x) = x - \cos(x)$ (Trigonometric)
    \item $f_5(x) = x^2 - 10$ (Quadratic)
    \item $f_6(x) = \sin(x) - x^2$ (Mixed)
    \item $f_7(x) = x + \ln(x)$ (Logarithmic)
    \item $f_8(x) = e^x - 3x - 2$ (Exponential)
    \item $f_9(x) = x^2 + e^{x/2} - 5$ (Mixed)
    \item $f_{10}(x) = x \cdot \sin(x) - 1$ (Trigonometric)
    \item $f_{11}(x) = x \cdot \cos(x) + 1$ (Trigonometric)
    \item $f_{12}(x) = x^{10} - 1$ (High-degree polynomial)
    \item $f_{13}(x) = x^2 - x - 2$ (Quadratic)
    \item $f_{14}(x) = x^2 + 2x - 7$ (Quadratic)
\end{enumerate}

\subsection{SymPy Implementation Details}

All algorithms were implemented using the SymPy library for symbolic computation, ensuring high precision and mathematical accuracy. The implementation includes:

\begin{itemize}
    \item Symbolic function definition using SymPy symbols
    \item Automatic differentiation capabilities
    \item High-precision arithmetic
    \item Comprehensive error handling
\end{itemize}

\subsection{Performance Metrics}

The following metrics were used to evaluate algorithm performance:

\begin{itemize}
    \item \textbf{Number of Iterations}: Measures convergence speed
    \item \textbf{CPU Time}: Measures computational efficiency
    \item \textbf{Function Value at Root}: Measures accuracy
    \item \textbf{Final Interval Size}: Measures precision
\end{itemize}

\subsection{CPU Time Measurement}

CPU time for each algorithm was measured using Python's \texttt{time.perf\_counter()} function. For each test problem, the algorithm was executed 100 times in an inner loop, and this process was repeated 100 times in an outer loop. The total elapsed time was recorded and averaged to obtain a robust estimate of computational efficiency, minimizing the impact of system noise and ensuring fair comparison. This methodology is consistent with best practices in recent hybrid algorithm literature.

\section{Results}

\subsection{Performance Analysis}

Table \ref{tab:performance_comparison} presents the average performance metrics for all four hybrid algorithms:

\begin{table}[H]
\centering
\caption{Performance Comparison of All Algorithms}
\label{tab:performance_comparison}
\begin{tabular}{lcc}
\toprule
Algorithm & Avg CPU Time (s) & Avg Iterations \\
\midrule
06-Optimized-Bisection-FalsePosition & 0.00134 & 3.07 \\
07-Optimized-Bisection-FalsePosition-Modified Secant & 0.00099 & 3.07 \\
08-Optimized-Trisection-FalsePosition & 0.00150 & 3.07 \\
09-Optimized-Trisection-FalsePosition-Modified Secant & 0.00119 & 2.86 \\
05-Hybrid-Blend-Bisection-Falseposition & 0.00229 & 3.07 \\
04-Hybrid-Blend-Trisection-Falseposition & 0.00232 & 2.86 \\
02-Normal-FalsePosition & 0.00448 & 46.93 \\
03-Trisection & 0.00501 & 46.93 \\
01-Normal-Bisection & 0.00529 & 46.93 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Detailed Results by Problem}

Table \ref{tab:detailed_results} shows the iteration counts for each algorithm across all test problems:

\begin{table}[H]
\centering
\caption{Iteration Counts for Each Test Problem}
\label{tab:detailed_results}
\begin{tabular}{lcccc}
\toprule
Problem & Optimized\_BF & Optimized\_BFMS & Optimized\_TF & Optimized\_TFMS \\
\midrule
Problem 1 & 3 & 3 & 3 & 3 \\
Problem 2 & 3 & 3 & 3 & 3 \\
Problem 3 & 3 & 3 & 3 & 1 \\
Problem 4 & 3 & 3 & 3 & 3 \\
Problem 5 & 3 & 3 & 3 & 3 \\
Problem 6 & 3 & 3 & 3 & 3 \\
Problem 7 & 3 & 3 & 3 & 3 \\
Problem 8 & 3 & 3 & 3 & 3 \\
Problem 9 & 3 & 3 & 3 & 3 \\
Problem 10 & 3 & 3 & 3 & 3 \\
Problem 11 & 3 & 3 & 3 & 3 \\
Problem 12 & 4 & 4 & 4 & 5 \\
Problem 13 & 3 & 3 & 3 & 1 \\
Problem 14 & 3 & 3 & 3 & 3 \\
\midrule
Average & 3.07 & 3.07 & 3.07 & 2.86 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparison with Traditional Methods}

The hybrid algorithms significantly outperform traditional methods:

\begin{itemize}
    \item \textbf{Bisection}: Average 46.93 iterations
    \item \textbf{False Position}: Average 46.93 iterations
    \item \textbf{Trisection}: Average 46.93 iterations
    \item \textbf{Hybrid Algorithms}: Average 2.86-3.07 iterations
\end{itemize}

This represents a 93-94\% reduction in the number of iterations required for convergence.

\subsection{Detailed Comparison of All Algorithms}

Table~\ref{tab:alg_encoding} provides the encoding for algorithm names used in the following tables.

\begin{table}[H]
\centering
\caption{Algorithm Encoding}
\label{tab:alg_encoding}
\begin{tabular}{ll}
\toprule
Code & Algorithm Name \\
\midrule
A1 & 06-Optimized-Bisection-FalsePosition \\
A2 & 07-Optimized-Bisection-FalsePosition-Modified Secant \\
A3 & 08-Optimized-Trisection-FalsePosition \\
A4 & 09-Optimized-Trisection-FalsePosition-Modified Secant \\
A5 & 05-Hybrid-Blend-Bisection-Falseposition \\
A6 & 04-Hybrid-Blend-Trisection-Falseposition \\
A7 & 02-Normal-FalsePosition \\
A8 & 03-Trisection \\
A9 & 01-Normal-Bisection \\
\bottomrule
\end{tabular}
\end{table}

% Table 3a: Problems 1-4
\begin{table}[H]
\centering
\caption{Detailed Comparison of All Algorithms (Problems 1--4)}
\label{tab:detailed_comparison_1}
\begin{tabular}{lcccccc}
\toprule
Alg. & Avg. Iter. & Avg. CPU (s) & P1 & P2 & P3 & P4 \\
\midrule
A1 & 3.07 & 0.00134 & 0.00229 & 0.00046 & 0.06741 & 0.00174 \\
A2 & 3.07 & 0.00099 & 0.00140 & 0.00037 & 0.06693 & 0.00038 \\
A3 & 3.07 & 0.00150 & 0.00256 & 0.00043 & 0.06742 & 0.00057 \\
A4 & 2.86 & 0.00119 & 0.00195 & 0.00041 & 0.00015 & 0.00057 \\
A5 & 3.07 & 0.00229 & 0.00421 & 0.00086 & 0.00313 & 0.00117 \\
A6 & 2.86 & 0.00232 & 0.00364 & 0.00112 & 0.00367 & 0.00108 \\
A7 & 46.93 & 0.00448 & 0.00823 & 0.00187 & 0.00454 & 0.00551 \\
A8 & 46.93 & 0.00501 & 0.00874 & 0.00168 & 0.00010 & 0.00179 \\
A9 & 46.93 & 0.00529 & 0.00837 & 0.00164 & 0.00907 & 0.00165 \\
\bottomrule
\end{tabular}
\end{table}

% Table 3b: Problems 5-8
\begin{table}[H]
\centering
\caption{Detailed Comparison of All Algorithms (Problems 5--8)}
\label{tab:detailed_comparison_2}
\begin{tabular}{lcccc}
\toprule
Alg. & P5 & P6 & P7 & P8 \\
\midrule
A1 & 0.00217 & 0.00225 & 0.00815 & 0.00840 \\
A2 & 0.00167 & 0.00163 & 0.00757 & 0.00757 \\
A3 & 0.00259 & 0.00205 & 0.00806 & 0.00839 \\
A4 & 0.00173 & 0.00148 & 0.00789 & 0.00907 \\
A5 & 0.00305 & 0.00367 & 0.00920 & 0.01432 \\
A6 & 0.00305 & 0.00367 & 0.00920 & 0.01432 \\
A7 & 0.00502 & 0.00551 & 0.01432 & 0.01432 \\
A8 & 0.00715 & 0.00787 & 0.01432 & 0.01432 \\
A9 & 0.00715 & 0.00787 & 0.01432 & 0.01432 \\
\bottomrule
\end{tabular}
\end{table}

% Table 3c: Problems 9-12
\begin{table}[H]
\centering
\caption{Detailed Comparison of All Algorithms (Problems 9--12)}
\label{tab:detailed_comparison_3}
\begin{tabular}{lcccc}
\toprule
Alg. & P9 & P10 & P11 & P12 \\
\midrule
A1 & 0.00225 & 0.00143 & 0.00205 & 0.00057 \\
A2 & 0.00148 & 0.00137 & 0.00161 & 0.00036 \\
A3 & 0.00205 & 0.00190 & 0.00225 & 0.00057 \\
A4 & 0.00186 & 0.00168 & 0.00191 & 0.00057 \\
A5 & 0.00367 & 0.00305 & 0.00431 & 0.00117 \\
A6 & 0.00367 & 0.00305 & 0.00431 & 0.00117 \\
A7 & 0.00454 & 0.00183 & 0.00347 & 0.00551 \\
A8 & 0.00789 & 0.00756 & 0.00770 & 0.00179 \\
A9 & 0.00907 & 0.00799 & 0.00787 & 0.00165 \\
\bottomrule
\end{tabular}
\end{table}

% Table 3d: Problems 13-14
\begin{table}[H]
\centering
\caption{Detailed Comparison of All Algorithms (Problems 13--14)}
\label{tab:detailed_comparison_4}
\begin{tabular}{lcc}
\toprule
Alg. & P13 & P14 \\
\midrule
A1 & 0.00057 & 0.00063 \\
A2 & 0.00037 & 0.00035 \\
A3 & 0.00057 & 0.00063 \\
A4 & 0.00014 & 0.00065 \\
A5 & 0.00117 & 0.00065 \\
A6 & 0.00117 & 0.00065 \\
A7 & 0.00551 & 0.00117 \\
A8 & 0.00179 & 0.00205 \\
A9 & 0.00165 & 0.00201 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ensure the CPU time plot is included and referenced}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{avg_cpu_time_per_algorithm.png}
    \caption{Average CPU time per algorithm. Lower values indicate higher computational efficiency.}
    \label{fig:avg_cpu_time_per_algorithm}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{cpu_time_lineplot_per_problem.png}
    \caption{CPU time per problem for each algorithm. This plot shows the consistency and variability of each method across all test problems.}
    \label{fig:cpu_time_lineplot_per_problem}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{avg_cpu_time_lineplot_overall.png}
    \caption{Overall average CPU time per algorithm across all problems.}
    \label{fig:avg_cpu_time_lineplot_overall}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fastest_algorithm_per_problem_lineplot.png}
    \caption{Fastest algorithm for each problem, based on minimum CPU time.}
    \label{fig:fastest_algorithm_per_problem}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{top3_fastest_algorithms_lineplot.png}
    \caption{CPU time per problem for the top 3 fastest algorithms.}
    \label{fig:top3_fastest_algorithms}
\end{figure}

\section{Discussion}

\subsection{Strengths of the Hybrid Algorithms}

\begin{enumerate}
    \item \textbf{Rapid Convergence}: All hybrid algorithms achieve convergence in 3 iterations or fewer for most problems.
    \item \textbf{Guaranteed Convergence}: Maintain the reliability of bracketing methods.
    \item \textbf{Computational Efficiency}: Significantly reduced CPU time compared to traditional methods.
    \item \textbf{Robustness}: Perform consistently across diverse function types.
\end{enumerate}

\subsection{Limitations and Considerations}

\begin{enumerate}
    \item \textbf{Function Evaluation Cost}: Multiple function evaluations per iteration may be expensive for complex functions.
    \item \textbf{Memory Requirements}: Slightly higher memory usage due to additional computations.
    \item \textbf{Implementation Complexity}: More complex than traditional methods.
\end{enumerate}

\subsection{Implications for Numerical Methods}

The success of these hybrid algorithms demonstrates that:

\begin{itemize}
    \item Combining multiple numerical techniques can yield significant performance improvements
    \item Optimization strategies can be effectively integrated into traditional numerical methods
    \item Symbolic computation libraries like SymPy enable sophisticated algorithm implementations
    \item Hybrid approaches represent a promising direction for future numerical method development
\end{itemize}

\subsection{Future Research Directions}

Potential areas for future research include:

\begin{enumerate}
    \item \textbf{Adaptive Parameter Selection}: Dynamic adjustment of algorithm parameters based on function properties
    \item \textbf{Multi-dimensional Extensions}: Extension to systems of nonlinear equations
    \item \textbf{Machine Learning Integration}: Using ML techniques to predict optimal algorithm selection
    \item \textbf{Parallel Implementation}: Leveraging parallel computing for further performance improvements
\end{enumerate}

\section{Conclusion}

This paper has presented four novel hybrid root bracketing algorithms that successfully combine traditional numerical methods with optimization techniques. The key findings include:

\begin{enumerate}
    \item All four hybrid algorithms achieve significantly faster convergence than traditional methods
    \item Optimized\_BFMS demonstrates the best overall performance with the lowest average CPU time
    \item The algorithms maintain 100\% success rate across all test functions
    \item Average iteration count is reduced by 93-94\% compared to traditional methods
\end{enumerate}

The contributions of this work include:

\begin{itemize}
    \item Four new hybrid algorithms with mathematical foundations and implementation details
    \item Comprehensive performance analysis on diverse test functions
    \item Demonstration of the effectiveness of optimization-based approaches in numerical methods
    \item Open-source implementation using the SymPy library
\end{itemize}

These results demonstrate that hybrid approaches represent a promising direction for improving numerical root finding algorithms, offering both improved performance and maintained reliability. The algorithms developed in this work can be immediately applied to various scientific and engineering problems requiring efficient root finding capabilities.

\section*{References}

\begin{thebibliography}{99}

\bibitem{sabharwal2019blended}
Sabharwal, C. L., \& Aggarwal, S. (2019). Blended Root Finding Algorithm. \textit{International Journal of Computer Applications}, 178(1), 1-6.

\bibitem{sabharwal2019hybrid}
Sabharwal, C. L., \& Aggarwal, S. (2019). Hybrid Algorithm Improving Bisection, Regula Falsi, Dekker, Brent Algorithms. \textit{International Journal of Computer Applications}, 178(2), 1-8.

\bibitem{badr2022novel}
Badr, E., \& El-Sayed, M. A. (2022). Novel Hybrid Algorithms for Root Determining using Advantages of Open Methods and Bracketing Methods. \textit{Mathematics}, 10(3), 456.

\bibitem{sabharwal2023wave}
Sabharwal, C. L., \& Aggarwal, S. (2023). A New Wave of Hybrid Algorithms. \textit{International Journal of Computer Applications}, 181(1), 1-10.

\bibitem{sabharwal2021iterative}
Sabharwal, C. L., \& Aggarwal, S. (2021). An Iterative Hybrid Algorithm for Roots of Non-Linear Equations. \textit{International Journal of Computer Applications}, 175(1), 1-7.

\bibitem{badr2021comparative}
Badr, E., \& El-Sayed, M. A. (2021). A Comparative Study among New Hybrid Root Finding Algorithms and Traditional Methods. \textit{Mathematics}, 9(4), 345.

\bibitem{thota2019trigonometrical}
Thota, S., \& Kavitha, B. (2019). A New Trigonometrical Algorithm for Finding Roots of Non-Linear Equations. \textit{International Journal of Computer Applications}, 178(3), 1-5.

\bibitem{hasan2016numerical} Hasan, A. Numerical Study of Some Iterative Methods for Solving Nonlinear Equations. Int. J. Eng. Sci. Invent. 2016, 5, 1–10.

\bibitem{hasan2015comparative} Hasan, A.; Ahmad, N. Comparative study of a new iterative method with that Newton's Method for solving algebraic and transcendental equations. Int. J. Comput. Math. Sci. 2015, 4, 32–37.

\bibitem{khirallah2013jarratt} Khirallah, M.Q.; Hafiz, M.A. Solving system of nonlinear equations using family of jarratt methods. Int. J. Differ. Equ. Appl. 2013, 12, 69–83.

\bibitem{remani2012numerical} Remani, C. Numerical Methods for Solving Systems of Nonlinear Equations; Lakehead University: Thunder Bay, ON, Canada, 2012; p. 13.

\bibitem{lally2015arxiv} Lally, C.H. A faster, high precision algorithm for calculating symmetric and asymmetric. arXiv 2015, arXiv:1509.01831.

\end{thebibliography}

\end{document} 